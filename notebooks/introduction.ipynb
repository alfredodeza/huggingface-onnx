{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— Hugging Face and ONNX introduction\n",
    "ONNX Runtime is a performance-focused inference engine for ONNX models. It is cross-platform and open source. It supports execution of models on a wide range of devices, from CPUs to GPUs.\n",
    "\n",
    "ONNX allows for interoperability between different frameworks and tools, it is open source and therefore can be easily extended, and it is efficient and scalable.\n",
    "\n",
    "This notebook will explore the ONNX runtime and how to use it with Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation \n",
    "Make sure you are using the right libraries. It might be easier to use conda to install the dependencies and runtime settings. For example:\n",
    "\n",
    "```yaml\n",
    "name: huggingface-onnx\n",
    "\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pytorch::pytorch\n",
    "  - pip\n",
    "  - transformers[onnx]\n",
    "  - pip:\n",
    "    - ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported with ready-made configurations\n",
    "There are a few models that are supported right away with configurations, which means that it will be easy to port an existing Hugging Face model over to ONNX.\n",
    "\n",
    "Some of the popular models are:\n",
    "\n",
    "- BART\n",
    "- BERT\n",
    "- Data2VecText\n",
    "- Data2VecVision\n",
    "- DistilBERT\n",
    "- OpenAI GPT-2\n",
    "- RoBERTa\n",
    "- T5\n",
    "- Whisper\n",
    "- YOLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m transformers.onnx --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "Configurations are important because they allow you correctly interact and map with the resulting model. Hugging Face already includes ready-to-use configurations for the popular models, so no need to figure out how that mapping should go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta import RobertaConfig, RobertaOnnxConfig\n",
    "config = RobertaConfig()\n",
    "onnx_config = RobertaOnnxConfig(config)\n",
    "print(list(onnx_config.inputs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested reading\n",
    "Learn more about some other options and other supported serialization to ONNX in the [Hugging Face Transformers Serialization](https://huggingface.co/docs/transformers/serialization) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('huggingface-onnx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94a981d60ba4438f99102977cad4a32617e73d4c97d7f8c3767755cac47237bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
